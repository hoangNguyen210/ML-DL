{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizaition\n",
    "def Normalization(X_train,X_test):\n",
    "    Xmin= np.min(X_train,axis=0)\n",
    "    Xmax =np.max(X_train,axis=0)\n",
    "    X_train=(X_train - Xmin)/(Xmax - Xmin)\n",
    "    X_test=(X_test - Xmin)/(Xmax - Xmin)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hãy xây dựng mô hình logistic regression bằng tất cả các features trong file heart, so sánh với thư viện sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-03/heart.csv\")\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X= data.iloc[:,:-1].values\n",
    "y= data.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)\n",
    "X_train_norm,X_test_norm = Normalization(X_train,X_test)\n",
    "\n",
    "# y_train=y_train.reshape([-1,1])\n",
    "# y_test=y_test.reshape([-1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "$N$ là số samples , $m$ là số features\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)}    &\\cdots   &x_m^{(1)}    \\\\\n",
    "1 & x_1^{(1)} & x_2^{(1)}    &\\cdots   &x_m^{(2)}    \\\\\n",
    "1 & \\cdots    & \\cdots       &\\cdots   &  \\\\\n",
    "1 & x_1^{(N)} & x_2^{(N)}     &\\cdots  &x_m^{(N)}    \\\\\n",
    "\\end{bmatrix} , \n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\cdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix} ,\n",
    "W = \\begin{bmatrix}\n",
    "\\omega_0 \\\\\n",
    "\\omega_1 \\\\\n",
    "\\cdots \\\\\n",
    "\\omega_m\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "$ \\hat{y} = \\sigma(XW) $ \n",
    "\n",
    "$ J = -\\dfrac{1}{N}\\sum\\limits_{i=1}^{N}\\Big[y_i  log(\\hat{y_i}) + (1-y_i) log(1 - \\hat{y_i})\\Big]$\n",
    "\n",
    "$\\dfrac{dJ}{dW} = X^T*(\\hat{y} - y)$\n",
    "\n",
    "Cập nhật $W$ : \n",
    "$$ W:= W - \\alpha *  \\dfrac{dJ}{dW}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid (x) :\n",
    "    '''\n",
    "    Tính hàm sigmoid (đầu ra dự đoán của bài toán Logistic Regression)\n",
    "    '''\n",
    "    return 1/(1+ np.exp(-x))\n",
    "    \n",
    "\n",
    "class Logistic_Regression :\n",
    "    '''\n",
    "    Logistic Regression là 1 thuật toán phân loại được dùng để gán các \n",
    "    đối tượng cho 1 tập hợp giá trị rời rạc (như 0, 1, 2, ...).\n",
    "    '''\n",
    "    def __init__(self,n_epoch , lr):\n",
    "        '''\n",
    "        Khởi tạo các tham số cho mô hình\n",
    "        Tham số :\n",
    "        n_epoch -- Số epoch mà ta muốn chạy\n",
    "        lr -- tốc độ học của thuật toán  \n",
    "        '''\n",
    "        self.n_epoch = n_epoch\n",
    "        self.lr = lr\n",
    "    \n",
    "    def fit(self,X_train,y_train) :\n",
    "        '''\n",
    "        Học W tốt nhất\n",
    "        Tham số :\n",
    "        X_train -- dữ liệu tập huấn luyện\n",
    "        y_train -- label của tập huấn luyện\n",
    "        '''\n",
    "        np.random.seed(5)\n",
    "        y_train = y_train.reshape(-1,1)\n",
    "        X_concat = np.concatenate([np.ones([X_train.shape[0],1]), X_train],axis = 1)\n",
    "        self.N = X_train.shape[0]\n",
    "        W = np.random.normal(0,1, size = (X_concat.shape[1],1))\n",
    "\n",
    "        for epoch in range(self.n_epoch) :\n",
    "            y_hat = sigmoid(np.dot(X_concat,W))\n",
    "            loss = -(1/self.N) *  np.sum(y_train * np.log(y_hat) + (1-y_train)*np.log(1-y_hat))\n",
    "\n",
    "            gradient = (1/self.N) * np.dot(X_concat.T, y_hat - y_train)\n",
    "            W = W - self.lr * gradient\n",
    "            if epoch % 1000 == 0 :\n",
    "                print('Epoch : {} ---------------- Loss : {}'.format(epoch,loss))\n",
    "        self.W = W\n",
    "                \n",
    "    def predict(self,X_test):\n",
    "        '''\n",
    "        Dự đoán label của tập kiểm tra từ W đã được học\n",
    "        Tham số : \n",
    "        X_test -- tập kiểm tra\n",
    "        Trả về : \n",
    "        Label của tập kiểm tra \n",
    "        '''\n",
    "        ones = np.ones((X_test.shape[0],1 ))\n",
    "        self.X_test_concat = np.concatenate((ones,X_test), axis = 1)\n",
    "        y_hat = sigmoid(np.dot(self.X_test_concat,self.W))\n",
    "        y_predict = np.where(y_hat > 0.5, 1,0)\n",
    "        return y_predict\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        '''\n",
    "        Tính score giữa label dự đoán và label thật\n",
    "        Tham số :\n",
    "        X_test -- tập kiểm tra (mô hình dự đoán label từ tập này)\n",
    "        y_test -- label thật \n",
    "        Trả về :\n",
    "        Số trong khoảng [0,1] nói về khả năng học của mô hình.\n",
    "        '''\n",
    "        y_test = y_test.reshape(-1,1)\n",
    "        ones = np.ones((X_test.shape[0],1 ))\n",
    "        self.X_test_concat = np.concatenate((ones,X_test), axis = 1)\n",
    "        y_hat = sigmoid(np.dot(self.X_test_concat,self.W))\n",
    "        y_predict = np.where(y_hat > 0.5, 1,0)\n",
    "        return np.mean(y_predict  == y_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ---------------- Loss : 1.285659017534689\n",
      "Epoch : 1000 ---------------- Loss : 0.34669271100798593\n",
      "Epoch : 2000 ---------------- Loss : 0.32535421610966303\n",
      "Epoch : 3000 ---------------- Loss : 0.3172479575630845\n",
      "Epoch : 4000 ---------------- Loss : 0.31315146825981977\n",
      "Epoch : 5000 ---------------- Loss : 0.3108319603053405\n",
      "Epoch : 6000 ---------------- Loss : 0.30942985649138305\n",
      "Epoch : 7000 ---------------- Loss : 0.30854322474929385\n",
      "Epoch : 8000 ---------------- Loss : 0.30796291879976656\n",
      "Epoch : 9000 ---------------- Loss : 0.30757235280743306\n",
      "Epoch : 10000 ---------------- Loss : 0.3073032482939276\n",
      "Epoch : 11000 ---------------- Loss : 0.30711406072652736\n",
      "Epoch : 12000 ---------------- Loss : 0.3069787108659649\n",
      "Epoch : 13000 ---------------- Loss : 0.3068803881859713\n",
      "Epoch : 14000 ---------------- Loss : 0.3068080031348633\n",
      "Epoch : 15000 ---------------- Loss : 0.3067540878683341\n",
      "Epoch : 16000 ---------------- Loss : 0.30671351902911875\n",
      "Epoch : 17000 ---------------- Loss : 0.3066827217104953\n",
      "Epoch : 18000 ---------------- Loss : 0.30665916260926424\n",
      "Epoch : 19000 ---------------- Loss : 0.30664102102265106\n",
      "Epoch : 20000 ---------------- Loss : 0.30662697147112006\n",
      "Epoch : 21000 ---------------- Loss : 0.30661603769005674\n",
      "Epoch : 22000 ---------------- Loss : 0.3066074930321505\n",
      "Epoch : 23000 ---------------- Loss : 0.3066007915310712\n",
      "Epoch : 24000 ---------------- Loss : 0.3065955195256338\n",
      "Epoch : 25000 ---------------- Loss : 0.3065913612679269\n",
      "Epoch : 26000 ---------------- Loss : 0.30658807417221484\n",
      "Epoch : 27000 ---------------- Loss : 0.30658547079720216\n",
      "Epoch : 28000 ---------------- Loss : 0.3065834055898644\n",
      "Epoch : 29000 ---------------- Loss : 0.3065817650366132\n",
      "Epoch : 30000 ---------------- Loss : 0.30658046028027747\n",
      "Epoch : 31000 ---------------- Loss : 0.3065794215405214\n",
      "Epoch : 32000 ---------------- Loss : 0.30657859386634106\n",
      "Epoch : 33000 ---------------- Loss : 0.30657793388151416\n",
      "Epoch : 34000 ---------------- Loss : 0.3065774072764446\n",
      "Epoch : 35000 ---------------- Loss : 0.3065769868653608\n",
      "Epoch : 36000 ---------------- Loss : 0.30657665107469834\n",
      "Epoch : 37000 ---------------- Loss : 0.3065763827623695\n",
      "Epoch : 38000 ---------------- Loss : 0.3065761682923501\n",
      "Epoch : 39000 ---------------- Loss : 0.30657599680723163\n",
      "Epoch : 40000 ---------------- Loss : 0.3065758596549254\n",
      "Epoch : 41000 ---------------- Loss : 0.30657574993585246\n",
      "Epoch : 42000 ---------------- Loss : 0.30657566214461573\n",
      "Epoch : 43000 ---------------- Loss : 0.30657559188597017\n",
      "Epoch : 44000 ---------------- Loss : 0.3065755356493669\n",
      "Epoch : 45000 ---------------- Loss : 0.30657549062977235\n",
      "Epoch : 46000 ---------------- Loss : 0.3065754545851133\n",
      "Epoch : 47000 ---------------- Loss : 0.306575425722759\n",
      "Epoch : 48000 ---------------- Loss : 0.3065754026090529\n",
      "Epoch : 49000 ---------------- Loss : 0.3065753840971673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Logistic_Regression( n_epoch = 50000 , lr = 0.1)\n",
    "model.fit(X_train_norm,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả chạy bằng mô hình :  0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Kết quả chạy bằng mô hình : ',model.score(X_test_norm,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả chạy bằng thư viện :  0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(\n",
    "    random_state=1000,\n",
    "    penalty= 'none'\n",
    ").fit(X_train_norm, y_train)\n",
    "\n",
    "print('Kết quả chạy bằng thư viện : ',clf.score(X_test_norm,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_norm,X_test_norm  = Normalization(X_train,X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Regression** : \n",
    "\n",
    "Đầu ra của bài toán Softmax khác với bài toán Logistic Regression ở hàm kích hoạt :\n",
    "\n",
    "$\\hat{y} = softmax(XW)$\n",
    "\n",
    "Loss : \n",
    "\n",
    "$L = -\\sum\\limits_{i=1}^{c}y_i log(\\hat{y_i})$ ($c$ là số class của bài toán)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(z):\n",
    "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "\n",
    "class Multiclass_LogisticRegression:\n",
    "    '''\n",
    "    Softmax Regression khác với bài toán Logistic Regression ở chỗ đầu ra sẽ có nhiều lớp hơn.\n",
    "    '''\n",
    "    def __init__(self, n_epoch, lr ) :\n",
    "        '''\n",
    "        Khởi tạo các tham số cho mô hình\n",
    "        Tham số :\n",
    "        n_epoch -- Số epoch mà ta muốn chạy\n",
    "        lr -- tốc độ học của thuật toán  \n",
    "        '''\n",
    "        self.n_epoch = n_epoch\n",
    "        self.lr = lr\n",
    "    \n",
    "    def fit(self, X_train,y_train):\n",
    "        '''\n",
    "        Chọn mô hình học W tốt nhất\n",
    "        Tham số :\n",
    "        X_train -- dữ liệu tập huấn luyện\n",
    "        y_train -- label của tập huấn luyện\n",
    "        '''\n",
    "        np.random.seed(5)\n",
    "        self.N = X_train.shape[0]\n",
    "        X_concat = np.concatenate([np.ones([self.N,1]), X_train],axis = 1)\n",
    "        W = np.random.normal(0,1, size = (X_concat.shape[1],y_train.max() + 1 ))\n",
    "        \n",
    "        y_train_onehot = np.zeros( (y_train.size, y_train.max() + 1),dtype=int)\n",
    "        y_train_onehot[np.arange(y_train.size), y_train.reshape(-1)] = 1\n",
    "        self.archived_W =  [W]   # Tạo ra list để lưu W\n",
    "        self.archived_loss = [10]  # tạo ra list để luu loss\n",
    "\n",
    "        for epochs in range(self.n_epoch):\n",
    "            y_hat = softmax(np.dot(X_concat,W))\n",
    "            loss = -(1/self.N) *  np.sum(y_train_onehot * np.log(y_hat))\n",
    "            self.archived_loss.append(loss)\n",
    "            gradient = (1/self.N) * np.dot(X_concat.T, y_hat - y_train_onehot)\n",
    "            W = W - self.lr * gradient\n",
    "            self.archived_W.append(W)\n",
    "            if epochs % 1000 == 0 :\n",
    "                print('Epoch {}  -----------  Loss {}'.format(epochs,loss))\n",
    "            self.W = W\n",
    "            \n",
    "    def predict(self,X_test) :\n",
    "        '''\n",
    "        Dự đoán label của tập kiểm tra từ W đã được học\n",
    "        Tham số : \n",
    "        X_test -- tập kiểm tra\n",
    "        Trả về : \n",
    "        Label của tập kiểm tra \n",
    "        '''\n",
    "        X_test_concat = np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis=1)\n",
    "        y_hat = softmax(np.dot(X_test_concat,self.W))\n",
    "        y_predict = np.argmax(y_hat,axis = 1)\n",
    "        return y_predict\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        '''\n",
    "        Tính score giữa label dự đoán và label thật\n",
    "        Tham số :\n",
    "        X_test -- tập kiểm tra (mô hình dự đoán label từ tập này)\n",
    "        y_test -- label thật \n",
    "        Trả về :\n",
    "        Số trong khoảng [0,1] nói về khả năng học của mô hình.\n",
    "        '''\n",
    "        self.X_test_concat = np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis=1)\n",
    "        y_hat = softmax(np.dot(self.X_test_concat,self.W))\n",
    "        y_predict = np.argmax(y_hat,axis = 1)\n",
    "        return np.mean(y_predict == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  -----------  Loss 2.108380248317868\n",
      "Epoch 1000  -----------  Loss 0.878363238759448\n",
      "Epoch 2000  -----------  Loss 0.691397045306415\n",
      "Epoch 3000  -----------  Loss 0.5966185035410634\n",
      "Epoch 4000  -----------  Loss 0.5369631571275945\n",
      "Epoch 5000  -----------  Loss 0.49459886201008657\n",
      "Epoch 6000  -----------  Loss 0.4622576318801885\n",
      "Epoch 7000  -----------  Loss 0.43637079127035777\n",
      "Epoch 8000  -----------  Loss 0.41494781727759183\n",
      "Epoch 9000  -----------  Loss 0.39677398754749404\n",
      "Epoch 10000  -----------  Loss 0.3810577062652737\n",
      "Epoch 11000  -----------  Loss 0.36725687634849297\n",
      "Epoch 12000  -----------  Loss 0.3549856098363435\n",
      "Epoch 13000  -----------  Loss 0.3439605408014571\n",
      "Epoch 14000  -----------  Loss 0.3339681874935225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Multiclass_LogisticRegression(n_epoch = 15000, lr = 0.01 )\n",
    "model.fit(X_train_norm,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả chạy bằng mô hình :  0.9\n"
     ]
    }
   ],
   "source": [
    "print('Kết quả chạy bằng mô hình : ',model.score(X_test_norm,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kêt quả chạy bằng thư viện : 0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train_norm, y_train)\n",
    "\n",
    "print('Kêt quả chạy bằng thư viện :', clf.score(X_test_norm,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
